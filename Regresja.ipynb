{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2fec36",
   "metadata": {},
   "source": [
    "# Problem regresji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f298814",
   "metadata": {},
   "source": [
    "Dodanie potrzenych bibliotek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033cedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d065f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27e749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb5a26",
   "metadata": {},
   "source": [
    "Podzielenie zaimportowanych danych na zbiór testowy i uczący się. W kazdym zbiorze znajdują się feature oraz label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b2ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd26c0",
   "metadata": {},
   "source": [
    "Wyświetlenie zbioru danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9554c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
       "        3.96900e+02, 1.87200e+01],\n",
       "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
       "        3.95380e+02, 3.11000e+00],\n",
       "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.75520e+02, 3.26000e+00],\n",
       "       ...,\n",
       "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
       "        3.62250e+02, 7.83000e+00],\n",
       "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.61950e+02, 1.57900e+01],\n",
       "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
       "        3.76700e+02, 4.38000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6cbeee",
   "metadata": {},
   "source": [
    "Nasz zbiór uczący sie naley nastepnie podzilic na zbiór treningowy i walidacyjny. Proporcje podziału\n",
    "dobrałam na zasadzie 85-15 ale nie jest to sztywna zasada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b9bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_full = train_features[:int(0.85*train_features.shape[0])]\n",
    "valid_features_full = train_features[int(0.85*train_features.shape[0]):]\n",
    "train_labels_full = train_labels[:int(0.85*train_features.shape[0]):]\n",
    "valid_labels_full = train_labels[int(0.85*train_features.shape[0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac252a6",
   "metadata": {},
   "source": [
    "Kiedy mamy podzielone dane możemy w końcu przejść do napisania sieci neuronowej.\n",
    "Używamy do tego celu biblioteki Keras z \n",
    "tensorflow. Sieć neuronową zaczynamy pisać poprzez odwołanie się do \n",
    "funkcji biblioteki Keraz. W argumentach podajemy warstwy sieci. Nastepnie kompilujemy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51bc234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 23:06:26.732013: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-15 23:06:26.732356: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mse',\n",
    "              metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75e841",
   "metadata": {},
   "source": [
    "Funkcja callbacks.ModelCheckpoint zapisuje najlepsze dopasowanie do pliku o nazwie model_regression.h5 Funkcja .fit\n",
    "trenuje model na zbiorze treningowym oraz waliduje na zbiorze walidacyjnym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ebb3407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 23:06:26.922837: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 23:06:27.248376: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 29ms/step - loss: 17603.8008 - mae: 128.0533 - mse: 17603.8008 - val_loss: 15212.4980 - val_mae: 119.0441 - val_mse: 15212.4980\n",
      "Epoch 2/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 11885.7500 - mae: 104.1655 - mse: 11885.7500 - val_loss: 10246.0645 - val_mae: 96.9306 - val_mse: 10246.0645\n",
      "Epoch 3/256\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 9404.2891 - mae: 92.3828 - mse: 9404.2891"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 23:06:28.230286: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 10ms/step - loss: 7705.8208 - mae: 83.7256 - mse: 7705.8208 - val_loss: 6606.7300 - val_mae: 77.9801 - val_mse: 6606.7300\n",
      "Epoch 4/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4848.9033 - mae: 65.7519 - mse: 4848.9033 - val_loss: 4096.0103 - val_mae: 60.6555 - val_mse: 4096.0103\n",
      "Epoch 5/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2992.8472 - mae: 50.4760 - mse: 2992.8472 - val_loss: 2574.0139 - val_mae: 46.9384 - val_mse: 2574.0139\n",
      "Epoch 6/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1962.4979 - mae: 39.3908 - mse: 1962.4979 - val_loss: 1690.8228 - val_mae: 37.4334 - val_mse: 1690.8228\n",
      "Epoch 7/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1407.3585 - mae: 32.5616 - mse: 1407.3585 - val_loss: 1225.8748 - val_mae: 30.6142 - val_mse: 1225.8748\n",
      "Epoch 8/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1093.1041 - mae: 27.3953 - mse: 1093.1041 - val_loss: 955.1625 - val_mae: 25.6811 - val_mse: 955.1625\n",
      "Epoch 9/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 877.0870 - mae: 23.3537 - mse: 877.0870 - val_loss: 771.0527 - val_mae: 22.1073 - val_mse: 771.0527\n",
      "Epoch 10/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 710.7833 - mae: 20.1567 - mse: 710.7833 - val_loss: 628.4265 - val_mae: 19.4686 - val_mse: 628.4265\n",
      "Epoch 11/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 572.2534 - mae: 17.5857 - mse: 572.2534 - val_loss: 518.9077 - val_mae: 17.3452 - val_mse: 518.9077\n",
      "Epoch 12/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 462.1569 - mae: 15.4888 - mse: 462.1569 - val_loss: 430.7624 - val_mae: 15.5383 - val_mse: 430.7624\n",
      "Epoch 13/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 374.2964 - mae: 13.6937 - mse: 374.2964 - val_loss: 360.0982 - val_mae: 13.9571 - val_mse: 360.0982\n",
      "Epoch 14/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 310.3387 - mae: 12.1549 - mse: 310.3387 - val_loss: 301.2139 - val_mae: 12.5184 - val_mse: 301.2139\n",
      "Epoch 15/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 254.9088 - mae: 10.8443 - mse: 254.9088 - val_loss: 254.5483 - val_mae: 11.2185 - val_mse: 254.5483\n",
      "Epoch 16/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 215.6272 - mae: 9.8434 - mse: 215.6272 - val_loss: 218.3249 - val_mae: 10.1558 - val_mse: 218.3249\n",
      "Epoch 17/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 184.0310 - mae: 9.0694 - mse: 184.0310 - val_loss: 191.2920 - val_mae: 9.3276 - val_mse: 191.2920\n",
      "Epoch 18/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 161.7380 - mae: 8.6146 - mse: 161.7380 - val_loss: 170.4702 - val_mae: 8.6231 - val_mse: 170.4702\n",
      "Epoch 19/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 145.0949 - mae: 8.2494 - mse: 145.0949 - val_loss: 155.5925 - val_mae: 8.1495 - val_mse: 155.5925\n",
      "Epoch 20/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 132.5090 - mae: 7.9920 - mse: 132.5090 - val_loss: 143.7619 - val_mae: 7.8212 - val_mse: 143.7619\n",
      "Epoch 21/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 123.2955 - mae: 7.8180 - mse: 123.2955 - val_loss: 134.9464 - val_mae: 7.5670 - val_mse: 134.9464\n",
      "Epoch 22/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 116.5410 - mae: 7.7159 - mse: 116.5410 - val_loss: 128.5554 - val_mae: 7.3921 - val_mse: 128.5554\n",
      "Epoch 23/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 112.0089 - mae: 7.6177 - mse: 112.0089 - val_loss: 124.2550 - val_mae: 7.2999 - val_mse: 124.2550\n",
      "Epoch 24/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 107.9790 - mae: 7.5200 - mse: 107.9790 - val_loss: 120.3990 - val_mae: 7.2322 - val_mse: 120.3990\n",
      "Epoch 25/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 104.9067 - mae: 7.5098 - mse: 104.9067 - val_loss: 116.4514 - val_mae: 7.1980 - val_mse: 116.4514\n",
      "Epoch 26/256\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 102.0548 - mae: 7.4575 - mse: 102.0548 - val_loss: 114.5444 - val_mae: 7.1386 - val_mse: 114.5444\n",
      "Epoch 27/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 100.0190 - mae: 7.4394 - mse: 100.0190 - val_loss: 111.6155 - val_mae: 7.1230 - val_mse: 111.6155\n",
      "Epoch 28/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 97.8041 - mae: 7.3771 - mse: 97.8041 - val_loss: 110.7237 - val_mae: 7.0463 - val_mse: 110.7237\n",
      "Epoch 29/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 96.4106 - mae: 7.2627 - mse: 96.4106 - val_loss: 110.1615 - val_mae: 6.9788 - val_mse: 110.1615\n",
      "Epoch 30/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 94.6995 - mae: 7.2947 - mse: 94.6995 - val_loss: 106.6654 - val_mae: 7.0008 - val_mse: 106.6654\n",
      "Epoch 31/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 92.8281 - mae: 7.2446 - mse: 92.8281 - val_loss: 106.0658 - val_mae: 6.9184 - val_mse: 106.0658\n",
      "Epoch 32/256\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 91.4292 - mae: 7.1693 - mse: 91.4292 - val_loss: 104.9483 - val_mae: 6.8666 - val_mse: 104.9483\n",
      "Epoch 33/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 90.0877 - mae: 7.0949 - mse: 90.0877 - val_loss: 104.2697 - val_mae: 6.7980 - val_mse: 104.2697\n",
      "Epoch 34/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 88.8640 - mae: 7.0608 - mse: 88.8640 - val_loss: 102.5247 - val_mae: 6.7762 - val_mse: 102.5247\n",
      "Epoch 35/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 87.7966 - mae: 7.0963 - mse: 87.7966 - val_loss: 100.7193 - val_mae: 6.7697 - val_mse: 100.7193\n",
      "Epoch 36/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 86.8839 - mae: 6.9781 - mse: 86.8839 - val_loss: 101.8081 - val_mae: 6.6334 - val_mse: 101.8081\n",
      "Epoch 37/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 85.3030 - mae: 6.9234 - mse: 85.3030 - val_loss: 99.0161 - val_mae: 6.6410 - val_mse: 99.0161\n",
      "Epoch 38/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 84.1006 - mae: 6.9290 - mse: 84.1006 - val_loss: 98.1205 - val_mae: 6.5945 - val_mse: 98.1205\n",
      "Epoch 39/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 83.1361 - mae: 6.8451 - mse: 83.1361 - val_loss: 98.2757 - val_mae: 6.5183 - val_mse: 98.2757\n",
      "Epoch 40/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 82.2374 - mae: 6.8123 - mse: 82.2374 - val_loss: 96.3673 - val_mae: 6.5287 - val_mse: 96.3673\n",
      "Epoch 41/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 81.2699 - mae: 6.8131 - mse: 81.2699 - val_loss: 95.7854 - val_mae: 6.4916 - val_mse: 95.7854\n",
      "Epoch 42/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 80.5202 - mae: 6.7201 - mse: 80.5202 - val_loss: 95.6889 - val_mae: 6.4238 - val_mse: 95.6889\n",
      "Epoch 43/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 79.6837 - mae: 6.6858 - mse: 79.6837 - val_loss: 94.6029 - val_mae: 6.4069 - val_mse: 94.6029\n",
      "Epoch 44/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 78.9130 - mae: 6.6828 - mse: 78.9130 - val_loss: 94.4402 - val_mae: 6.3504 - val_mse: 94.4402\n",
      "Epoch 45/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 78.1890 - mae: 6.6109 - mse: 78.1890 - val_loss: 92.9395 - val_mae: 6.3413 - val_mse: 92.9395\n",
      "Epoch 46/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 77.4094 - mae: 6.5981 - mse: 77.4094 - val_loss: 92.5645 - val_mae: 6.2803 - val_mse: 92.5645\n",
      "Epoch 47/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 76.6414 - mae: 6.5867 - mse: 76.6414 - val_loss: 90.8711 - val_mae: 6.2877 - val_mse: 90.8711\n",
      "Epoch 48/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 76.0081 - mae: 6.5381 - mse: 76.0081 - val_loss: 91.6777 - val_mae: 6.1828 - val_mse: 91.6777\n",
      "Epoch 49/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 75.2893 - mae: 6.4705 - mse: 75.2893 - val_loss: 90.2078 - val_mae: 6.1693 - val_mse: 90.2078\n",
      "Epoch 50/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 74.9099 - mae: 6.3866 - mse: 74.9099 - val_loss: 90.1833 - val_mae: 6.1086 - val_mse: 90.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 74.2335 - mae: 6.4712 - mse: 74.2335 - val_loss: 87.4686 - val_mae: 6.1596 - val_mse: 87.4686\n",
      "Epoch 52/256\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 73.3985 - mae: 6.4302 - mse: 73.3985 - val_loss: 88.7003 - val_mae: 6.0393 - val_mse: 88.7003\n",
      "Epoch 53/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 72.9810 - mae: 6.3406 - mse: 72.9810 - val_loss: 87.3262 - val_mae: 6.0305 - val_mse: 87.3262\n",
      "Epoch 54/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 72.3546 - mae: 6.2902 - mse: 72.3546 - val_loss: 88.0710 - val_mae: 5.9552 - val_mse: 88.0710\n",
      "Epoch 55/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 71.9714 - mae: 6.3264 - mse: 71.9714 - val_loss: 84.9574 - val_mae: 6.0135 - val_mse: 84.9574\n",
      "Epoch 56/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 71.3168 - mae: 6.2986 - mse: 71.3168 - val_loss: 86.0142 - val_mae: 5.9086 - val_mse: 86.0142\n",
      "Epoch 57/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 70.7012 - mae: 6.2559 - mse: 70.7012 - val_loss: 85.0669 - val_mae: 5.8901 - val_mse: 85.0669\n",
      "Epoch 58/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 70.6449 - mae: 6.1432 - mse: 70.6449 - val_loss: 85.8785 - val_mae: 5.8302 - val_mse: 85.8785\n",
      "Epoch 59/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 70.0232 - mae: 6.2429 - mse: 70.0232 - val_loss: 83.3819 - val_mae: 5.8560 - val_mse: 83.3819\n",
      "Epoch 60/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 69.1240 - mae: 6.1950 - mse: 69.1240 - val_loss: 83.8411 - val_mae: 5.7963 - val_mse: 83.8411\n",
      "Epoch 61/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 69.0685 - mae: 6.1215 - mse: 69.0685 - val_loss: 83.0285 - val_mae: 5.7740 - val_mse: 83.0285\n",
      "Epoch 62/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 68.3200 - mae: 6.0745 - mse: 68.3200 - val_loss: 83.1323 - val_mae: 5.7325 - val_mse: 83.1323\n",
      "Epoch 63/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 68.2887 - mae: 6.0296 - mse: 68.2887 - val_loss: 83.1931 - val_mae: 5.6996 - val_mse: 83.1931\n",
      "Epoch 64/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 68.1668 - mae: 6.1871 - mse: 68.1668 - val_loss: 80.1364 - val_mae: 5.7629 - val_mse: 80.1364\n",
      "Epoch 65/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 67.3331 - mae: 6.0738 - mse: 67.3331 - val_loss: 82.5792 - val_mae: 5.6406 - val_mse: 82.5792\n",
      "Epoch 66/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 66.8888 - mae: 5.9993 - mse: 66.8888 - val_loss: 80.9344 - val_mae: 5.6356 - val_mse: 80.9344\n",
      "Epoch 67/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 66.3792 - mae: 5.9670 - mse: 66.3792 - val_loss: 80.8505 - val_mae: 5.6028 - val_mse: 80.8505\n",
      "Epoch 68/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 66.2647 - mae: 6.0401 - mse: 66.2647 - val_loss: 79.1593 - val_mae: 5.6231 - val_mse: 79.1593\n",
      "Epoch 69/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 65.4811 - mae: 5.9691 - mse: 65.4811 - val_loss: 80.3982 - val_mae: 5.5629 - val_mse: 80.3982\n",
      "Epoch 70/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 65.4952 - mae: 5.8439 - mse: 65.4952 - val_loss: 80.4189 - val_mae: 5.5498 - val_mse: 80.4189\n",
      "Epoch 71/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 64.8801 - mae: 5.9160 - mse: 64.8801 - val_loss: 77.5500 - val_mae: 5.5943 - val_mse: 77.5500\n",
      "Epoch 72/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 64.8186 - mae: 5.9939 - mse: 64.8186 - val_loss: 78.4191 - val_mae: 5.5117 - val_mse: 78.4191\n",
      "Epoch 73/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 64.1575 - mae: 5.8085 - mse: 64.1575 - val_loss: 79.2839 - val_mae: 5.4943 - val_mse: 79.2839\n",
      "Epoch 74/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 64.3877 - mae: 5.8740 - mse: 64.3877 - val_loss: 76.8655 - val_mae: 5.5082 - val_mse: 76.8655\n",
      "Epoch 75/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 63.7197 - mae: 5.7717 - mse: 63.7197 - val_loss: 78.4746 - val_mae: 5.4647 - val_mse: 78.4746\n",
      "Epoch 76/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 63.6669 - mae: 5.8478 - mse: 63.6669 - val_loss: 75.8430 - val_mae: 5.4951 - val_mse: 75.8430\n",
      "Epoch 77/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 63.8077 - mae: 5.7899 - mse: 63.8077 - val_loss: 78.6251 - val_mae: 5.4375 - val_mse: 78.6251\n",
      "Epoch 78/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 62.7740 - mae: 5.7517 - mse: 62.7740 - val_loss: 75.4097 - val_mae: 5.4617 - val_mse: 75.4097\n",
      "Epoch 79/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 62.3178 - mae: 5.8216 - mse: 62.3178 - val_loss: 75.2399 - val_mae: 5.4407 - val_mse: 75.2399\n",
      "Epoch 80/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 62.0842 - mae: 5.7318 - mse: 62.0842 - val_loss: 76.1849 - val_mae: 5.3963 - val_mse: 76.1849\n",
      "Epoch 81/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 61.8460 - mae: 5.7566 - mse: 61.8460 - val_loss: 73.9704 - val_mae: 5.4485 - val_mse: 73.9704\n",
      "Epoch 82/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 61.7659 - mae: 5.7546 - mse: 61.7659 - val_loss: 76.2751 - val_mae: 5.3618 - val_mse: 76.2751\n",
      "Epoch 83/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 61.2334 - mae: 5.6542 - mse: 61.2334 - val_loss: 74.1184 - val_mae: 5.3878 - val_mse: 74.1184\n",
      "Epoch 84/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 60.9051 - mae: 5.7008 - mse: 60.9051 - val_loss: 74.6329 - val_mae: 5.3593 - val_mse: 74.6329\n",
      "Epoch 85/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 60.8319 - mae: 5.7240 - mse: 60.8319 - val_loss: 73.1226 - val_mae: 5.3818 - val_mse: 73.1226\n",
      "Epoch 86/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 60.2754 - mae: 5.6376 - mse: 60.2754 - val_loss: 74.0610 - val_mae: 5.3436 - val_mse: 74.0610\n",
      "Epoch 87/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 60.0464 - mae: 5.5800 - mse: 60.0464 - val_loss: 73.4937 - val_mae: 5.3383 - val_mse: 73.4937\n",
      "Epoch 88/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 59.8282 - mae: 5.6122 - mse: 59.8282 - val_loss: 73.0707 - val_mae: 5.3373 - val_mse: 73.0707\n",
      "Epoch 89/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 59.6085 - mae: 5.5482 - mse: 59.6085 - val_loss: 73.0859 - val_mae: 5.3272 - val_mse: 73.0859\n",
      "Epoch 90/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 59.4697 - mae: 5.6411 - mse: 59.4697 - val_loss: 71.9247 - val_mae: 5.3424 - val_mse: 71.9247\n",
      "Epoch 91/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 59.5519 - mae: 5.7061 - mse: 59.5519 - val_loss: 70.9276 - val_mae: 5.3616 - val_mse: 70.9276\n",
      "Epoch 92/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 58.8389 - mae: 5.5389 - mse: 58.8389 - val_loss: 72.2720 - val_mae: 5.3011 - val_mse: 72.2720\n",
      "Epoch 93/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 58.6793 - mae: 5.4882 - mse: 58.6793 - val_loss: 71.5592 - val_mae: 5.3031 - val_mse: 71.5592\n",
      "Epoch 94/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 58.2637 - mae: 5.5253 - mse: 58.2637 - val_loss: 70.9896 - val_mae: 5.3074 - val_mse: 70.9896\n",
      "Epoch 95/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 58.0413 - mae: 5.5017 - mse: 58.0413 - val_loss: 70.8681 - val_mae: 5.2966 - val_mse: 70.8681\n",
      "Epoch 96/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 57.9378 - mae: 5.5113 - mse: 57.9378 - val_loss: 69.8981 - val_mae: 5.3104 - val_mse: 69.8981\n",
      "Epoch 97/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 57.5997 - mae: 5.4539 - mse: 57.5997 - val_loss: 71.1171 - val_mae: 5.2585 - val_mse: 71.1171\n",
      "Epoch 98/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 57.3036 - mae: 5.4555 - mse: 57.3036 - val_loss: 68.8613 - val_mae: 5.3160 - val_mse: 68.8613\n",
      "Epoch 99/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 57.2640 - mae: 5.4589 - mse: 57.2640 - val_loss: 70.0417 - val_mae: 5.2593 - val_mse: 70.0417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 56.9682 - mae: 5.4761 - mse: 56.9682 - val_loss: 68.5321 - val_mae: 5.2941 - val_mse: 68.5321\n",
      "Epoch 101/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 57.1638 - mae: 5.3968 - mse: 57.1638 - val_loss: 69.0082 - val_mae: 5.2623 - val_mse: 69.0082\n",
      "Epoch 102/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 56.3870 - mae: 5.4223 - mse: 56.3870 - val_loss: 68.0190 - val_mae: 5.2846 - val_mse: 68.0190\n",
      "Epoch 103/256\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.2239 - mae: 5.4064 - mse: 56.2239 - val_loss: 68.7427 - val_mae: 5.2502 - val_mse: 68.7427\n",
      "Epoch 104/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 55.9619 - mae: 5.4203 - mse: 55.9619 - val_loss: 67.4641 - val_mae: 5.2740 - val_mse: 67.4641\n",
      "Epoch 105/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 56.2615 - mae: 5.3284 - mse: 56.2615 - val_loss: 68.5991 - val_mae: 5.2405 - val_mse: 68.5991\n",
      "Epoch 106/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 55.6241 - mae: 5.4016 - mse: 55.6241 - val_loss: 66.5507 - val_mae: 5.2802 - val_mse: 66.5507\n",
      "Epoch 107/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 55.4769 - mae: 5.4003 - mse: 55.4769 - val_loss: 67.1296 - val_mae: 5.2484 - val_mse: 67.1296\n",
      "Epoch 108/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 55.1114 - mae: 5.3617 - mse: 55.1114 - val_loss: 67.3032 - val_mae: 5.2368 - val_mse: 67.3032\n",
      "Epoch 109/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 55.0805 - mae: 5.2465 - mse: 55.0805 - val_loss: 68.3694 - val_mae: 5.2276 - val_mse: 68.3694\n",
      "Epoch 110/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 54.9800 - mae: 5.3436 - mse: 54.9800 - val_loss: 65.6121 - val_mae: 5.2538 - val_mse: 65.6121\n",
      "Epoch 111/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 54.6586 - mae: 5.3328 - mse: 54.6586 - val_loss: 68.0703 - val_mae: 5.2240 - val_mse: 68.0703\n",
      "Epoch 112/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 54.3972 - mae: 5.2250 - mse: 54.3972 - val_loss: 66.3744 - val_mae: 5.2219 - val_mse: 66.3744\n",
      "Epoch 113/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 54.1404 - mae: 5.2882 - mse: 54.1404 - val_loss: 65.4870 - val_mae: 5.2249 - val_mse: 65.4870\n",
      "Epoch 114/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 54.0574 - mae: 5.2731 - mse: 54.0574 - val_loss: 65.0703 - val_mae: 5.2228 - val_mse: 65.0703\n",
      "Epoch 115/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 53.7487 - mae: 5.2631 - mse: 53.7487 - val_loss: 65.1130 - val_mae: 5.2153 - val_mse: 65.1130\n",
      "Epoch 116/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 54.5199 - mae: 5.4525 - mse: 54.5199 - val_loss: 64.6617 - val_mae: 5.2135 - val_mse: 64.6617\n",
      "Epoch 117/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 53.8491 - mae: 5.1053 - mse: 53.8491 - val_loss: 66.9915 - val_mae: 5.2057 - val_mse: 66.9915\n",
      "Epoch 118/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 53.4380 - mae: 5.2691 - mse: 53.4380 - val_loss: 63.1509 - val_mae: 5.2250 - val_mse: 63.1509\n",
      "Epoch 119/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 53.0898 - mae: 5.2143 - mse: 53.0898 - val_loss: 65.4041 - val_mae: 5.1974 - val_mse: 65.4041\n",
      "Epoch 120/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 53.4045 - mae: 5.2746 - mse: 53.4045 - val_loss: 64.2697 - val_mae: 5.1915 - val_mse: 64.2697\n",
      "Epoch 121/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 52.5680 - mae: 5.1334 - mse: 52.5680 - val_loss: 64.9098 - val_mae: 5.1839 - val_mse: 64.9098\n",
      "Epoch 122/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 52.6893 - mae: 5.2266 - mse: 52.6893 - val_loss: 63.5830 - val_mae: 5.1840 - val_mse: 63.5830\n",
      "Epoch 123/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 52.3205 - mae: 5.1240 - mse: 52.3205 - val_loss: 64.1707 - val_mae: 5.1826 - val_mse: 64.1707\n",
      "Epoch 124/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 52.0491 - mae: 5.1971 - mse: 52.0491 - val_loss: 62.3062 - val_mae: 5.1820 - val_mse: 62.3062\n",
      "Epoch 125/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 52.0120 - mae: 5.2066 - mse: 52.0120 - val_loss: 63.2487 - val_mae: 5.1702 - val_mse: 63.2487\n",
      "Epoch 126/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 51.7296 - mae: 5.1355 - mse: 51.7296 - val_loss: 63.8844 - val_mae: 5.1765 - val_mse: 63.8844\n",
      "Epoch 127/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 51.6385 - mae: 5.1120 - mse: 51.6385 - val_loss: 62.5965 - val_mae: 5.1745 - val_mse: 62.5965\n",
      "Epoch 128/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 51.8557 - mae: 5.0547 - mse: 51.8557 - val_loss: 62.2047 - val_mae: 5.1673 - val_mse: 62.2047\n",
      "Epoch 129/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 51.2356 - mae: 5.2476 - mse: 51.2356 - val_loss: 60.5371 - val_mae: 5.1660 - val_mse: 60.5371\n",
      "Epoch 130/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 51.0896 - mae: 5.1145 - mse: 51.0896 - val_loss: 62.5981 - val_mae: 5.1462 - val_mse: 62.5981\n",
      "Epoch 131/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 50.8366 - mae: 5.0434 - mse: 50.8366 - val_loss: 62.3425 - val_mae: 5.1554 - val_mse: 62.3425\n",
      "Epoch 132/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 50.6742 - mae: 5.0658 - mse: 50.6742 - val_loss: 62.1760 - val_mae: 5.1516 - val_mse: 62.1760\n",
      "Epoch 133/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 50.5173 - mae: 5.1050 - mse: 50.5173 - val_loss: 60.7546 - val_mae: 5.1425 - val_mse: 60.7546\n",
      "Epoch 134/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 50.5173 - mae: 5.0529 - mse: 50.5173 - val_loss: 61.9577 - val_mae: 5.1657 - val_mse: 61.9577\n",
      "Epoch 135/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 50.2958 - mae: 5.0976 - mse: 50.2958 - val_loss: 60.8260 - val_mae: 5.1458 - val_mse: 60.8260\n",
      "Epoch 136/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 49.9397 - mae: 5.0531 - mse: 49.9397 - val_loss: 61.0688 - val_mae: 5.1386 - val_mse: 61.0688\n",
      "Epoch 137/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 49.8196 - mae: 4.9965 - mse: 49.8196 - val_loss: 60.7725 - val_mae: 5.1368 - val_mse: 60.7725\n",
      "Epoch 138/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 49.6120 - mae: 5.0458 - mse: 49.6120 - val_loss: 59.5972 - val_mae: 5.1223 - val_mse: 59.5972\n",
      "Epoch 139/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 49.8307 - mae: 5.0165 - mse: 49.8307 - val_loss: 60.6938 - val_mae: 5.1399 - val_mse: 60.6938\n",
      "Epoch 140/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 49.2708 - mae: 5.0426 - mse: 49.2708 - val_loss: 59.0905 - val_mae: 5.1181 - val_mse: 59.0905\n",
      "Epoch 141/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 49.2515 - mae: 5.0498 - mse: 49.2515 - val_loss: 59.3288 - val_mae: 5.1153 - val_mse: 59.3288\n",
      "Epoch 142/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 49.7944 - mae: 4.9873 - mse: 49.7944 - val_loss: 59.6816 - val_mae: 5.1178 - val_mse: 59.6816\n",
      "Epoch 143/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 48.9451 - mae: 5.0811 - mse: 48.9451 - val_loss: 57.7404 - val_mae: 5.0925 - val_mse: 57.7404\n",
      "Epoch 144/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 49.3868 - mae: 4.9558 - mse: 49.3868 - val_loss: 60.1652 - val_mae: 5.1203 - val_mse: 60.1652\n",
      "Epoch 145/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 49.2302 - mae: 5.0947 - mse: 49.2302 - val_loss: 58.2775 - val_mae: 5.0972 - val_mse: 58.2775\n",
      "Epoch 146/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 48.4008 - mae: 4.9307 - mse: 48.4008 - val_loss: 59.5377 - val_mae: 5.1250 - val_mse: 59.5377\n",
      "Epoch 147/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 48.4558 - mae: 4.9697 - mse: 48.4558 - val_loss: 58.9117 - val_mae: 5.1127 - val_mse: 58.9117\n",
      "Epoch 148/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 48.1745 - mae: 4.9021 - mse: 48.1745 - val_loss: 58.3904 - val_mae: 5.1068 - val_mse: 58.3904\n",
      "Epoch 149/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 48.6301 - mae: 5.1017 - mse: 48.6301 - val_loss: 57.6980 - val_mae: 5.0934 - val_mse: 57.6980\n",
      "Epoch 150/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 48.7040 - mae: 4.8910 - mse: 48.7040 - val_loss: 59.0033 - val_mae: 5.1224 - val_mse: 59.0033\n",
      "Epoch 151/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 48.4316 - mae: 5.0460 - mse: 48.4316 - val_loss: 57.8107 - val_mae: 5.0980 - val_mse: 57.8107\n",
      "Epoch 152/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 47.6261 - mae: 4.8625 - mse: 47.6261 - val_loss: 59.0019 - val_mae: 5.1212 - val_mse: 59.0019\n",
      "Epoch 153/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 47.3725 - mae: 4.9048 - mse: 47.3725 - val_loss: 56.7227 - val_mae: 5.0804 - val_mse: 56.7227\n",
      "Epoch 154/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 47.3281 - mae: 4.9818 - mse: 47.3281 - val_loss: 57.1277 - val_mae: 5.0811 - val_mse: 57.1277\n",
      "Epoch 155/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 47.5714 - mae: 4.8392 - mse: 47.5714 - val_loss: 57.5192 - val_mae: 5.0977 - val_mse: 57.5192\n",
      "Epoch 156/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 47.0928 - mae: 5.0087 - mse: 47.0928 - val_loss: 55.5716 - val_mae: 5.0689 - val_mse: 55.5716\n",
      "Epoch 157/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 47.0163 - mae: 4.8725 - mse: 47.0163 - val_loss: 57.5945 - val_mae: 5.0878 - val_mse: 57.5945\n",
      "Epoch 158/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 46.6179 - mae: 4.8838 - mse: 46.6179 - val_loss: 55.9942 - val_mae: 5.0548 - val_mse: 55.9942\n",
      "Epoch 159/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 46.6917 - mae: 4.9539 - mse: 46.6917 - val_loss: 56.9885 - val_mae: 5.0650 - val_mse: 56.9885\n",
      "Epoch 160/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 46.6418 - mae: 4.8405 - mse: 46.6418 - val_loss: 56.4427 - val_mae: 5.0582 - val_mse: 56.4427\n",
      "Epoch 161/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 46.3459 - mae: 4.8739 - mse: 46.3459 - val_loss: 56.2959 - val_mae: 5.0693 - val_mse: 56.2959\n",
      "Epoch 162/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 46.2772 - mae: 4.8024 - mse: 46.2772 - val_loss: 56.2784 - val_mae: 5.0782 - val_mse: 56.2784\n",
      "Epoch 163/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 46.0244 - mae: 4.8221 - mse: 46.0244 - val_loss: 54.5430 - val_mae: 5.0428 - val_mse: 54.5430\n",
      "Epoch 164/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 46.2750 - mae: 5.0218 - mse: 46.2750 - val_loss: 54.6108 - val_mae: 5.0391 - val_mse: 54.6108\n",
      "Epoch 165/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 45.6633 - mae: 4.7570 - mse: 45.6633 - val_loss: 57.5475 - val_mae: 5.0697 - val_mse: 57.5475\n",
      "Epoch 166/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 45.7596 - mae: 4.8324 - mse: 45.7596 - val_loss: 54.3089 - val_mae: 5.0187 - val_mse: 54.3089\n",
      "Epoch 167/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 45.3271 - mae: 4.8166 - mse: 45.3271 - val_loss: 55.9190 - val_mae: 5.0481 - val_mse: 55.9190\n",
      "Epoch 168/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 45.8601 - mae: 4.7212 - mse: 45.8601 - val_loss: 54.5736 - val_mae: 5.0419 - val_mse: 54.5736\n",
      "Epoch 169/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 46.0859 - mae: 5.0392 - mse: 46.0859 - val_loss: 54.0994 - val_mae: 5.0270 - val_mse: 54.0994\n",
      "Epoch 170/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 44.9181 - mae: 4.7482 - mse: 44.9181 - val_loss: 57.1514 - val_mae: 5.0659 - val_mse: 57.1514\n",
      "Epoch 171/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 45.0216 - mae: 4.7562 - mse: 45.0216 - val_loss: 53.5870 - val_mae: 5.0175 - val_mse: 53.5870\n",
      "Epoch 172/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 44.9110 - mae: 4.8264 - mse: 44.9110 - val_loss: 53.9959 - val_mae: 5.0042 - val_mse: 53.9959\n",
      "Epoch 173/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 45.0431 - mae: 4.8237 - mse: 45.0431 - val_loss: 55.5374 - val_mae: 5.0317 - val_mse: 55.5374\n",
      "Epoch 174/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 44.7875 - mae: 4.7853 - mse: 44.7875 - val_loss: 53.5873 - val_mae: 5.0080 - val_mse: 53.5873\n",
      "Epoch 175/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 44.8549 - mae: 4.7044 - mse: 44.8549 - val_loss: 54.6457 - val_mae: 5.0269 - val_mse: 54.6457\n",
      "Epoch 176/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 46.4435 - mae: 5.1049 - mse: 46.4435 - val_loss: 52.2548 - val_mae: 4.9926 - val_mse: 52.2548\n",
      "Epoch 177/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 47.3134 - mae: 4.7404 - mse: 47.3134 - val_loss: 55.3220 - val_mae: 5.0224 - val_mse: 55.3220\n",
      "Epoch 178/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 43.9788 - mae: 4.8536 - mse: 43.9788 - val_loss: 51.2125 - val_mae: 4.9670 - val_mse: 51.2125\n",
      "Epoch 179/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 44.2251 - mae: 4.8431 - mse: 44.2251 - val_loss: 54.3741 - val_mae: 5.0002 - val_mse: 54.3741\n",
      "Epoch 180/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.9694 - mae: 4.6709 - mse: 43.9694 - val_loss: 53.6856 - val_mae: 4.9908 - val_mse: 53.6856\n",
      "Epoch 181/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.8423 - mae: 4.7588 - mse: 43.8423 - val_loss: 51.5183 - val_mae: 4.9749 - val_mse: 51.5183\n",
      "Epoch 182/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.8271 - mae: 4.8492 - mse: 43.8271 - val_loss: 52.9724 - val_mae: 4.9893 - val_mse: 52.9724\n",
      "Epoch 183/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.9000 - mae: 4.6759 - mse: 43.9000 - val_loss: 52.6685 - val_mae: 4.9751 - val_mse: 52.6685\n",
      "Epoch 184/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.9714 - mae: 4.8839 - mse: 43.9714 - val_loss: 51.4685 - val_mae: 4.9653 - val_mse: 51.4685\n",
      "Epoch 185/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.7028 - mae: 4.7067 - mse: 43.7028 - val_loss: 53.5500 - val_mae: 4.9994 - val_mse: 53.5500\n",
      "Epoch 186/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.4462 - mae: 4.6542 - mse: 43.4462 - val_loss: 51.7807 - val_mae: 4.9583 - val_mse: 51.7807\n",
      "Epoch 187/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.3860 - mae: 4.7063 - mse: 43.3860 - val_loss: 52.3698 - val_mae: 4.9742 - val_mse: 52.3698\n",
      "Epoch 188/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 43.7673 - mae: 4.8431 - mse: 43.7673 - val_loss: 51.1548 - val_mae: 4.9473 - val_mse: 51.1548\n",
      "Epoch 189/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.7607 - mae: 4.6786 - mse: 42.7607 - val_loss: 55.3436 - val_mae: 5.0250 - val_mse: 55.3436\n",
      "Epoch 190/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 43.3334 - mae: 4.6154 - mse: 43.3334 - val_loss: 49.6136 - val_mae: 4.9350 - val_mse: 49.6136\n",
      "Epoch 191/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.4348 - mae: 4.8703 - mse: 43.4348 - val_loss: 53.8774 - val_mae: 4.9747 - val_mse: 53.8774\n",
      "Epoch 192/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.6423 - mae: 4.6453 - mse: 43.6423 - val_loss: 50.4546 - val_mae: 4.9311 - val_mse: 50.4546\n",
      "Epoch 193/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.9562 - mae: 4.7705 - mse: 42.9562 - val_loss: 50.6548 - val_mae: 4.9377 - val_mse: 50.6548\n",
      "Epoch 194/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 43.1287 - mae: 4.6204 - mse: 43.1287 - val_loss: 51.6553 - val_mae: 4.9373 - val_mse: 51.6553\n",
      "Epoch 195/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 44.5909 - mae: 4.9714 - mse: 44.5909 - val_loss: 53.2030 - val_mae: 4.9688 - val_mse: 53.2030\n",
      "Epoch 196/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 8ms/step - loss: 42.4041 - mae: 4.6095 - mse: 42.4041 - val_loss: 51.8185 - val_mae: 4.9296 - val_mse: 51.8185\n",
      "Epoch 197/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.0987 - mae: 4.6749 - mse: 42.0987 - val_loss: 49.8741 - val_mae: 4.9030 - val_mse: 49.8741\n",
      "Epoch 198/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.2526 - mae: 4.7539 - mse: 42.2526 - val_loss: 52.3267 - val_mae: 4.9372 - val_mse: 52.3267\n",
      "Epoch 199/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.4876 - mae: 4.5819 - mse: 42.4876 - val_loss: 49.7326 - val_mae: 4.9236 - val_mse: 49.7326\n",
      "Epoch 200/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.0772 - mae: 4.7473 - mse: 42.0772 - val_loss: 50.1605 - val_mae: 4.9336 - val_mse: 50.1605\n",
      "Epoch 201/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.0207 - mae: 4.6331 - mse: 42.0207 - val_loss: 51.0196 - val_mae: 4.9139 - val_mse: 51.0196\n",
      "Epoch 202/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.8372 - mae: 4.7510 - mse: 41.8372 - val_loss: 49.8779 - val_mae: 4.9094 - val_mse: 49.8779\n",
      "Epoch 203/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.7632 - mae: 4.6110 - mse: 41.7632 - val_loss: 50.5286 - val_mae: 4.8990 - val_mse: 50.5286\n",
      "Epoch 204/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.9340 - mae: 4.7484 - mse: 41.9340 - val_loss: 51.0623 - val_mae: 4.9123 - val_mse: 51.0623\n",
      "Epoch 205/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.3935 - mae: 4.5803 - mse: 41.3935 - val_loss: 50.2355 - val_mae: 4.9012 - val_mse: 50.2355\n",
      "Epoch 206/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 41.3772 - mae: 4.7299 - mse: 41.3772 - val_loss: 48.6677 - val_mae: 4.8810 - val_mse: 48.6677\n",
      "Epoch 207/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.3981 - mae: 4.6001 - mse: 41.3981 - val_loss: 52.0504 - val_mae: 4.9468 - val_mse: 52.0504\n",
      "Epoch 208/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.6563 - mae: 4.8798 - mse: 42.6563 - val_loss: 49.8182 - val_mae: 4.8761 - val_mse: 49.8182\n",
      "Epoch 209/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.8391 - mae: 4.5270 - mse: 41.8391 - val_loss: 50.1081 - val_mae: 4.8911 - val_mse: 50.1081\n",
      "Epoch 210/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.2396 - mae: 4.7319 - mse: 41.2396 - val_loss: 48.9282 - val_mae: 4.8674 - val_mse: 48.9282\n",
      "Epoch 211/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.0641 - mae: 4.5353 - mse: 41.0641 - val_loss: 51.1869 - val_mae: 4.9256 - val_mse: 51.1869\n",
      "Epoch 212/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.8546 - mae: 4.6608 - mse: 40.8546 - val_loss: 48.5954 - val_mae: 4.8711 - val_mse: 48.5954\n",
      "Epoch 213/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.9173 - mae: 4.6164 - mse: 40.9173 - val_loss: 49.2553 - val_mae: 4.8731 - val_mse: 49.2553\n",
      "Epoch 214/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.6512 - mae: 4.6257 - mse: 40.6512 - val_loss: 48.4324 - val_mae: 4.8526 - val_mse: 48.4324\n",
      "Epoch 215/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.7774 - mae: 4.6525 - mse: 40.7774 - val_loss: 49.1338 - val_mae: 4.8804 - val_mse: 49.1338\n",
      "Epoch 216/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.6227 - mae: 4.7146 - mse: 40.6227 - val_loss: 49.2904 - val_mae: 4.8774 - val_mse: 49.2904\n",
      "Epoch 217/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.5309 - mae: 4.5599 - mse: 40.5309 - val_loss: 51.4054 - val_mae: 4.9469 - val_mse: 51.4054\n",
      "Epoch 218/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.5748 - mae: 4.6490 - mse: 40.5748 - val_loss: 46.5958 - val_mae: 4.8432 - val_mse: 46.5958\n",
      "Epoch 219/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.2803 - mae: 4.6136 - mse: 40.2803 - val_loss: 51.7951 - val_mae: 4.9635 - val_mse: 51.7951\n",
      "Epoch 220/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.4064 - mae: 4.6364 - mse: 40.4064 - val_loss: 47.9195 - val_mae: 4.8539 - val_mse: 47.9195\n",
      "Epoch 221/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.9947 - mae: 4.6230 - mse: 39.9947 - val_loss: 49.1882 - val_mae: 4.8836 - val_mse: 49.1882\n",
      "Epoch 222/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.4839 - mae: 4.5333 - mse: 40.4839 - val_loss: 47.1147 - val_mae: 4.8325 - val_mse: 47.1147\n",
      "Epoch 223/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.5445 - mae: 4.7813 - mse: 40.5445 - val_loss: 48.6161 - val_mae: 4.8684 - val_mse: 48.6161\n",
      "Epoch 224/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.8621 - mae: 4.5978 - mse: 39.8621 - val_loss: 49.4502 - val_mae: 4.8850 - val_mse: 49.4502\n",
      "Epoch 225/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.0612 - mae: 4.4953 - mse: 40.0612 - val_loss: 49.1922 - val_mae: 4.8765 - val_mse: 49.1922\n",
      "Epoch 226/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.2871 - mae: 4.7480 - mse: 40.2871 - val_loss: 47.5272 - val_mae: 4.8343 - val_mse: 47.5272\n",
      "Epoch 227/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.4413 - mae: 4.5749 - mse: 39.4413 - val_loss: 49.9512 - val_mae: 4.9136 - val_mse: 49.9512\n",
      "Epoch 228/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 40.7353 - mae: 4.5579 - mse: 40.7353 - val_loss: 46.4037 - val_mae: 4.8335 - val_mse: 46.4037\n",
      "Epoch 229/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.9953 - mae: 4.6850 - mse: 39.9953 - val_loss: 49.8479 - val_mae: 4.9167 - val_mse: 49.8479\n",
      "Epoch 230/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.9254 - mae: 4.6231 - mse: 39.9254 - val_loss: 46.8929 - val_mae: 4.8172 - val_mse: 46.8929\n",
      "Epoch 231/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.7182 - mae: 4.5800 - mse: 39.7182 - val_loss: 48.4777 - val_mae: 4.8667 - val_mse: 48.4777\n",
      "Epoch 232/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.3562 - mae: 4.5628 - mse: 39.3562 - val_loss: 48.6503 - val_mae: 4.8805 - val_mse: 48.6503\n",
      "Epoch 233/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.2902 - mae: 4.5929 - mse: 39.2902 - val_loss: 47.4204 - val_mae: 4.8422 - val_mse: 47.4204\n",
      "Epoch 234/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.3628 - mae: 4.5623 - mse: 39.3628 - val_loss: 48.5147 - val_mae: 4.8749 - val_mse: 48.5147\n",
      "Epoch 235/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.4033 - mae: 4.6352 - mse: 39.4033 - val_loss: 48.4907 - val_mae: 4.8796 - val_mse: 48.4907\n",
      "Epoch 236/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.1531 - mae: 4.5194 - mse: 39.1531 - val_loss: 47.2998 - val_mae: 4.8393 - val_mse: 47.2998\n",
      "Epoch 237/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.2389 - mae: 4.7017 - mse: 39.2389 - val_loss: 47.4482 - val_mae: 4.8447 - val_mse: 47.4482\n",
      "Epoch 238/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.4821 - mae: 4.4825 - mse: 39.4821 - val_loss: 48.0536 - val_mae: 4.8699 - val_mse: 48.0536\n",
      "Epoch 239/256\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 38.9985 - mae: 4.6136 - mse: 38.9985 - val_loss: 45.6336 - val_mae: 4.8062 - val_mse: 45.6336\n",
      "Epoch 240/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.7412 - mae: 4.6985 - mse: 39.7412 - val_loss: 49.9076 - val_mae: 4.9170 - val_mse: 49.9076\n",
      "Epoch 241/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.1641 - mae: 4.5826 - mse: 39.1641 - val_loss: 46.0672 - val_mae: 4.8034 - val_mse: 46.0672\n",
      "Epoch 242/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.9893 - mae: 4.5710 - mse: 38.9893 - val_loss: 47.6230 - val_mae: 4.8541 - val_mse: 47.6230\n",
      "Epoch 243/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.8097 - mae: 4.5839 - mse: 38.8097 - val_loss: 47.0142 - val_mae: 4.8441 - val_mse: 47.0142\n",
      "Epoch 244/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.6313 - mae: 4.5770 - mse: 38.6313 - val_loss: 47.3811 - val_mae: 4.8489 - val_mse: 47.3811\n",
      "Epoch 245/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.9894 - mae: 4.5704 - mse: 38.9894 - val_loss: 45.9231 - val_mae: 4.7958 - val_mse: 45.9231\n",
      "Epoch 246/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.5271 - mae: 4.6480 - mse: 38.5271 - val_loss: 49.2182 - val_mae: 4.9054 - val_mse: 49.2182\n",
      "Epoch 247/256\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 39.7368 - mae: 4.5679 - mse: 39.7368 - val_loss: 45.5908 - val_mae: 4.7965 - val_mse: 45.5908\n",
      "Epoch 248/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.6138 - mae: 4.5790 - mse: 38.6138 - val_loss: 46.8101 - val_mae: 4.8349 - val_mse: 46.8101\n",
      "Epoch 249/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.4063 - mae: 4.6022 - mse: 38.4063 - val_loss: 48.7901 - val_mae: 4.9003 - val_mse: 48.7901\n",
      "Epoch 250/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.8542 - mae: 4.4487 - mse: 38.8542 - val_loss: 46.0648 - val_mae: 4.8179 - val_mse: 46.0648\n",
      "Epoch 251/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.1065 - mae: 4.7454 - mse: 39.1065 - val_loss: 49.1802 - val_mae: 4.9057 - val_mse: 49.1802\n",
      "Epoch 252/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.7508 - mae: 4.4316 - mse: 38.7508 - val_loss: 46.2808 - val_mae: 4.8178 - val_mse: 46.2808\n",
      "Epoch 253/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.5561 - mae: 4.6463 - mse: 38.5561 - val_loss: 45.9334 - val_mae: 4.7976 - val_mse: 45.9334\n",
      "Epoch 254/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.4419 - mae: 4.6291 - mse: 38.4419 - val_loss: 48.4425 - val_mae: 4.8867 - val_mse: 48.4425\n",
      "Epoch 255/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.9205 - mae: 4.5380 - mse: 38.9205 - val_loss: 46.5018 - val_mae: 4.8332 - val_mse: 46.5018\n",
      "Epoch 256/256\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 38.0967 - mae: 4.5266 - mse: 38.0967 - val_loss: 47.1077 - val_mae: 4.8551 - val_mse: 47.1077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14fd0ddf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\"model_regression.h5\", save_best_only=True)\n",
    "model.fit(x = train_features_full, y = train_labels_full, validation_data=(valid_features_full, valid_labels_full), epochs=256, verbose=1, callbacks= checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e93f90e",
   "metadata": {},
   "source": [
    "Podczas trenowania po zakończeniu każdej z epok wyświetlane są statystyki dotyczące funkcji loss \n",
    "gdzie los dotyczy zbioru treningowego a val_los zbioru walidacyjnego. Gdzie mae (Mean absolute error) a \n",
    "mse (Mean Squared Error)\n",
    "\n",
    "Wyświetlamy wyniki dla pierwszych 5 wartości:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b708f795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.2 18.8 19.  27.  22.2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7a6cce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.204886 21.774988 23.163446 26.868504 24.375319]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 23:06:52.736537: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_features)\n",
    "print(predictions[:5].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa651f",
   "metadata": {},
   "source": [
    "Jak widać wyniki są zadowalające.\n",
    "Następnie zapisujemy dane z 0 próbki do pliku csv aby potem przetestować na nich działanie sieci w matlabie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc457e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 18.0846,   0.    ,  18.1   ,   0.    ,   0.679 ,   6.434 ,\n",
       "       100.    ,   1.8347,  24.    , 666.    ,  20.2   ,  27.25  ,\n",
       "        29.05  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "510ee3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('test.csv', test_features[0], delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnenv",
   "language": "python",
   "name": "nnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
